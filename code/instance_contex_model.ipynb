{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模思路\n",
    "\n",
    "类比语言模型的建立，可以建立一个物体共现模型：\n",
    "\n",
    "* 将一幅正常图像中所有共同出现的物体定义为集合I。那么I中任意一个元素，将会与I中其它元素经常出现在同一个图像中。和语言模型中词汇在上下文中出现的概率类似。那么我们可以训练一个神经网络模型M，表达I中各个元素共同出现的概率。\n",
    "\n",
    "* 当给定一个待判定集合S，使用模型M预测S中所有元素的出现概率，将预测结果中概率最小的元素，判定为不和谐元素。\n",
    "\n",
    "\n",
    "\n",
    "## 训练数据概览\n",
    "\n",
    "* 基于MSCOCO数据集 train2017 中 instance detecion 数据\n",
    "\n",
    "* 包含物体类别总共 80 个\n",
    "\n",
    "* 共有图片118287个\n",
    "\n",
    "* 每幅图像中包含物体类别的数量分布：\n",
    "\n",
    "\t\t(0, 3]      84213\n",
    "\t\t(3, 6]      27039\n",
    "\t\t(6, 9]       5165\n",
    "\t\t(9, 12]       774\n",
    "\t\t(12, 15]       72\n",
    "\t\t(15, 18]        3\n",
    "\t\t(18, 21]        0\n",
    "\t\t(21, 24]        0\n",
    "\t\t(24, 27]        0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0    # 用于填充的分类索引\n",
    "UNK_IDX = 91   # 不在coco已知分类中的物体索引\n",
    "\n",
    "VOCAB_SIZE = 92  # 所有分类的个数\n",
    "EMBEDDING_SIZE = 256  \n",
    "CONTEXT_SIZE = 64  # 每幅图片中物体数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoCatIds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, \\\n",
    "              24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, \\\n",
    "              48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, \\\n",
    "              72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocoCatIds[33-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 因为coco的category id是不连续的，为了防止中间不存在的id干扰loss计算，设置掩码把它们排除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(shape=(VOCAB_SIZE,), dtype=np.float32)\n",
    "for catId in cocoCatIds:\n",
    "    mask[catId] = 1.\n",
    "mask[PAD_IDX] = 1.\n",
    "mask[UNK_IDX] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "\n",
    "* 词表的建立：\n",
    "  * 直接以coco category id作为词表索引，但是这个id是不连续的，对于不在category id集合中的索引，通过mask将其中计算中屏蔽掉。\n",
    "  * PAD索引为0。\n",
    "  * UNK索引为91。\n",
    "\n",
    "* 输入层：每幅图片中物体类别数（CONTEXT_SIZE）。\n",
    "\n",
    "* embedding层：92 * 256，这里设置词嵌入长度为256。\n",
    "\n",
    "* 全联接隐层：256 * 256，tanh为激活函数。\n",
    "\n",
    "* 输出层：256 * 92，输出为词表大小的向量，sigmoid为激活函数（因为输出表示该类别出现的概率），不需要计算softmax。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_graph(learning_rate=1e-3):\n",
    "    # 在训练时，输入的是单个物体类别，输出是与其共现的所有其它物体类别的出现概率\n",
    "    # 在预测时，输入的是图片中所有物体，输出是与当前物体共现的所有其它物体类别的出现概率\n",
    "    inputs_ = tf.placeholder(tf.int32, shape=[None, None])\n",
    "    labels_ = tf.placeholder(tf.float32, shape=[None, VOCAB_SIZE])\n",
    "    \n",
    "    # embedding层\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBEDDING_SIZE]))\n",
    "    input_embeddings = tf.nn.embedding_lookup(embeddings, inputs_)\n",
    "    context_embeddings = tf.reduce_sum(input_embeddings, axis=1)\n",
    "    \n",
    "    # hidden层\n",
    "    hidden_output = tf.layers.dense(context_embeddings, EMBEDDING_SIZE, activation=tf.nn.tanh)\n",
    "    \n",
    "    # output层\n",
    "    raw_output = tf.layers.dense(hidden_output, VOCAB_SIZE)\n",
    "    final_output = tf.sigmoid(raw_output)\n",
    "    \n",
    "    # loss and GD optimizer\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=raw_output, labels=labels_)*mask\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels_) * mask\n",
    "    )\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return init, inputs_, labels_, final_output, loss, train_step, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据从输入为单个物体类别，转换为图片包含的所有物体类别，实际就是增加了PADDING。将原来的N*1维数组，变成了N*CONTEXT_SIZE维数组\n",
    "\n",
    "def prepare_data(raw_data):\n",
    "    data = np.zeros(shape=(len(raw_data), CONTEXT_SIZE), dtype=np.int32)\n",
    "    for i in range(len(raw_data)):\n",
    "        for j in range(len(raw_data[i])):\n",
    "            if j == CONTEXT_SIZE:\n",
    "                break;\n",
    "            data[i][j] = raw_data[i][j]\n",
    "    return data\n",
    "\n",
    "def prepare_labels(raw_labels):\n",
    "    labels = np.zeros(shape=(len(raw_labels), VOCAB_SIZE), dtype=np.float32)\n",
    "    for i in range(len(raw_labels)):\n",
    "        for item in raw_labels[i]:\n",
    "            labels[i][item] = 1.\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdata = np.load(\"data/train_data.npy\")\n",
    "train_data = prepare_data(train_rdata)\n",
    "train_rlabels = np.load(\"data/train_labels.npy\")\n",
    "train_labels = prepare_labels(train_rlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 1, 28]), array([57,  1]), array([28, 57]), array([ 1, 35]),\n",
       "       array([35, 27]), array([27,  1]), array([18, 34]), array([ 1, 34]),\n",
       "       array([ 1, 18]), array([67,  5,  8, 41, 15, 27, 31])], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rlabels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(session, inputs_, labels_, final_output, loss, train_step):\n",
    "    batch_size = 32\n",
    "    epochs = 1\n",
    "    average_loss = 0\n",
    "    step = 0\n",
    "    eval_interval = 1000\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            num_inputs = len(train_data)\n",
    "            order = np.arange(num_inputs)\n",
    "            np.random.shuffle(order)\n",
    "            for j in range(0, num_inputs, batch_size):\n",
    "                step += 1\n",
    "                batch_index = order[j: j + batch_size]\n",
    "                batch_inputs = train_data[batch_index]\n",
    "                batch_labels = train_labels[batch_index]\n",
    "                feed_dict = {inputs_: batch_inputs, \\\n",
    "                             labels_: batch_labels}\n",
    "                # We perform one update step by evaluating the optimizer op (including it\n",
    "                # in the list of returned values for session.run()\n",
    "                _, loss_val = session.run([train_step, loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "                # if step == 100:\n",
    "                #     break\n",
    "    \n",
    "                if step % eval_interval == 0:\n",
    "                    average_loss /= eval_interval\n",
    "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print('\\ntime: {:.2f} s'.format(end_time - start_time))\n",
    "\n",
    "def model_save(session, saver, model_base_path = './models/', model_name = 'instance-context-model'):\n",
    "    saver.save(session, os.path.join(model_base_path, model_name))\n",
    "\n",
    "def model_restore(session, saver, model_base_path = './models/', model_name = 'instance-context-model'):\n",
    "    saver.restore(session, os.path.join(model_base_path, model_name))\n",
    "    \n",
    "def model_inference(session, inputs, inputs_ph, final_output):\n",
    "    feed_dict = {inputs_ph: inputs}\n",
    "    output_val = session.run([final_output], feed_dict=feed_dict)\n",
    "    \n",
    "    results_list = list()\n",
    "    for res in output_val:\n",
    "        for item in res:\n",
    "            results_list.append(item)\n",
    "    \n",
    "    r = list(results_list[0]*mask)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    learning_rate = 1e-3\n",
    "    init, inputs_, labels_, \\\n",
    "    final_output, loss, \\\n",
    "    train_step, saver = build_model_graph(learning_rate)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print('Initialized')\n",
    "        model_train(session, inputs_, labels_, final_output, loss, train_step)\n",
    "        model_save(session, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  1000 :  0.479146612257\n",
      "Average loss at step  2000 :  0.267900686741\n",
      "Average loss at step  3000 :  0.202884733021\n",
      "Average loss at step  4000 :  0.174836337477\n",
      "Average loss at step  5000 :  0.1604286796\n",
      "Average loss at step  6000 :  0.150988770217\n",
      "Average loss at step  7000 :  0.145265809089\n",
      "Average loss at step  8000 :  0.142057618134\n",
      "Average loss at step  9000 :  0.138996910848\n",
      "\n",
      "time: 69.89 s\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_catIds(classIds):\n",
    "    catIds = list()\n",
    "    for classId in classIds:\n",
    "        catIds.append(cocoCatIds[classId - 1])\n",
    "    return catIds\n",
    "\n",
    "def validate_batch(session, batch_inputs, inputs_ph, final_output):\n",
    "    rets = list()\n",
    "    for single_input in batch_inputs:\n",
    "        ret = validate_one(session, list(single_input), inputs_ph)\n",
    "        rets.append(ret)\n",
    "    return rets\n",
    "\n",
    "def validate_one(session, ins_cat_ids, inputs_ph, final_output):\n",
    "    all_catIds = set(ins_cat_ids)\n",
    "    catIds = []\n",
    "    inputs = np.zeros(shape=(1, CONTEXT_SIZE), dtype=np.int32)\n",
    "    for i in range(len(ins_cat_ids)):\n",
    "        inputs[0][i] = ins_cat_ids[i]\n",
    "    \n",
    "    while len(catIds) != len(all_catIds):\n",
    "        r = model_inference(session, inputs, inputs_ph, final_output)\n",
    "        candidates = dict()\n",
    "        for catId in all_catIds:\n",
    "            if not (catId in catIds):\n",
    "                candidates[catId] = r[catId]\n",
    "                # print (catId, r[catId])\n",
    "        newcid = max(candidates, key=candidates.get)\n",
    "        # print (newcid)\n",
    "        # print (\"*\"*30)\n",
    "        catIds += [newcid]\n",
    "        inputs = np.zeros(shape=(1, CONTEXT_SIZE), dtype=np.int32)\n",
    "        for i in range(len(ins_cat_ids)):\n",
    "            if ins_cat_ids[i] in catIds:\n",
    "                inputs[0][i] = ins_cat_ids[i]\n",
    "    \n",
    "    # print (catIds)\n",
    "    return catIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/instance-context-model\n",
      "[1, 37, 25]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 1e-3\n",
    "init, inputs_, labels_, \\\n",
    "final_output, loss, \\\n",
    "train_step, saver = build_model_graph(learning_rate)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    model_restore(session, saver)\n",
    "    ins_class_ids = [ 1,  1,  1, 33, 24,  1,  1]\n",
    "    ins_cat_ids = get_coco_catIds(ins_class_ids)\n",
    "    ret = validate_one(ins_cat_ids)\n",
    "    print (ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
