{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模思路\n",
    "\n",
    "类比语言模型的建立，可以建立一个物体共现模型：\n",
    "\n",
    "* 将一幅正常图像中所有共同出现的物体定义为集合I。那么I中任意一个元素，将会与I中其它元素经常出现在同一个图像中。和语言模型中词汇在上下文中出现的概率类似。那么我们可以训练一个神经网络模型M，表达I中各个元素共同出现的概率。\n",
    "\n",
    "* 当给定一个待判定集合S，使用模型M预测S中所有元素的出现概率，将预测结果中概率最小的元素，判定为不和谐元素。\n",
    "\n",
    "\n",
    "\n",
    "## 训练数据概览\n",
    "\n",
    "* 基于MSCOCO数据集 train2017 中 instance detecion 数据\n",
    "\n",
    "* 包含物体类别总共 80 个\n",
    "\n",
    "* 共有图片118287个\n",
    "\n",
    "* 每幅图像中包含物体类别的数量分布：\n",
    "\n",
    "\t\t(0, 3]      84213\n",
    "\t\t(3, 6]      27039\n",
    "\t\t(6, 9]       5165\n",
    "\t\t(9, 12]       774\n",
    "\t\t(12, 15]       72\n",
    "\t\t(15, 18]        3\n",
    "\t\t(18, 21]        0\n",
    "\t\t(21, 24]        0\n",
    "\t\t(24, 27]        0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0    # 用于填充的分类索引\n",
    "UNK_IDX = 91   # 不在coco已知分类中的物体索引\n",
    "\n",
    "VOCAB_SIZE = 92  # 所有分类的个数\n",
    "EMBEDDING_SIZE = 256  \n",
    "CONTEXT_SIZE = 16  # 每幅图片中物体类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoCatIds = \\\n",
    "[1,\n",
    " 2,\n",
    " 3,\n",
    " 4,\n",
    " 5,\n",
    " 6,\n",
    " 7,\n",
    " 8,\n",
    " 9,\n",
    " 10,\n",
    " 11,\n",
    " 13,\n",
    " 14,\n",
    " 15,\n",
    " 16,\n",
    " 17,\n",
    " 18,\n",
    " 19,\n",
    " 20,\n",
    " 21,\n",
    " 22,\n",
    " 23,\n",
    " 24,\n",
    " 25,\n",
    " 27,\n",
    " 28,\n",
    " 31,\n",
    " 32,\n",
    " 33,\n",
    " 34,\n",
    " 35,\n",
    " 36,\n",
    " 37,\n",
    " 38,\n",
    " 39,\n",
    " 40,\n",
    " 41,\n",
    " 42,\n",
    " 43,\n",
    " 44,\n",
    " 46,\n",
    " 47,\n",
    " 48,\n",
    " 49,\n",
    " 50,\n",
    " 51,\n",
    " 52,\n",
    " 53,\n",
    " 54,\n",
    " 55,\n",
    " 56,\n",
    " 57,\n",
    " 58,\n",
    " 59,\n",
    " 60,\n",
    " 61,\n",
    " 62,\n",
    " 63,\n",
    " 64,\n",
    " 65,\n",
    " 67,\n",
    " 70,\n",
    " 72,\n",
    " 73,\n",
    " 74,\n",
    " 75,\n",
    " 76,\n",
    " 77,\n",
    " 78,\n",
    " 79,\n",
    " 80,\n",
    " 81,\n",
    " 82,\n",
    " 84,\n",
    " 85,\n",
    " 86,\n",
    " 87,\n",
    " 88,\n",
    " 89,\n",
    " 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 因为coco的category id是不连续的，为了防止中间不存在的id干扰loss计算，设置掩码把它们排除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(shape=(VOCAB_SIZE,), dtype=np.float32)\n",
    "for catId in cocoCatIds:\n",
    "    mask[catId] = 1.\n",
    "mask[PAD_IDX] = 1.\n",
    "mask[UNK_IDX] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "\n",
    "* 词表的建立：\n",
    "  * 直接以coco category id作为词表索引，但是这个id是不连续的，对于不在category id集合中的索引，通过mask将其中计算中屏蔽掉。\n",
    "  * PAD索引为0。\n",
    "  * UNK索引为91。\n",
    "\n",
    "* 输入层：每幅图片中物体类别数（CONTEXT_SIZE）。\n",
    "\n",
    "* embedding层：92 * 256，这里设置词嵌入长度为256。\n",
    "\n",
    "* 全联接隐层：256 * 256，tanh为激活函数。\n",
    "\n",
    "* 输出层：256 * 92，输出为词表大小的向量，sigmoid为激活函数（因为输出表示该类别出现的概率），不需要计算softmax。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.int32, shape=[None, CONTEXT_SIZE])\n",
    "labels_ = tf.placeholder(tf.float32, shape=[None, VOCAB_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding层\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBEDDING_SIZE]))\n",
    "input_embeddings = tf.nn.embedding_lookup(embeddings, inputs_)\n",
    "context_embeddings = tf.reduce_sum(input_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden层\n",
    "hidden_output = tf.layers.dense(context_embeddings, EMBEDDING_SIZE, activation=tf.nn.tanh)\n",
    "\n",
    "# output层\n",
    "raw_output = tf.layers.dense(hidden_output, VOCAB_SIZE)\n",
    "final_output = tf.sigmoid(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid:0' shape=(?, 92) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and GD optimizer\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=raw_output, labels=labels_)*mask\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels_) * mask\n",
    ")\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据从输入为单个物体类别，转换为图片包含的所有物体类别，实际就是增加了PADDING。将原来的N*1维数组，变成了N*CONTEXT_SIZE维数组\n",
    "\n",
    "def prepare_data(raw_data):\n",
    "    data = np.zeros(shape=(len(raw_data), CONTEXT_SIZE), dtype=np.int32)\n",
    "    for i in range(len(raw_data)):\n",
    "        for j in range(len(raw_data[i])):\n",
    "            if j == 16:\n",
    "                break;\n",
    "            data[i][j] = raw_data[i][j]\n",
    "    return data\n",
    "\n",
    "def prepare_labels(raw_labels):\n",
    "    labels = np.zeros(shape=(len(raw_labels), VOCAB_SIZE), dtype=np.float32)\n",
    "    for i in range(len(raw_labels)):\n",
    "        for item in raw_labels[i]:\n",
    "            labels[i][item] = 1.\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdata = np.load(\"data/train_data.npy\")\n",
    "train_data = prepare_data(train_rdata)\n",
    "train_rlabels = np.load(\"data/train_labels.npy\")\n",
    "train_labels = prepare_labels(train_rlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at epoch  1000 :  0.189834122762\n",
      "Average loss at epoch  2000 :  0.131263718776\n",
      "Interrupted\n",
      "\n",
      "time: 12.18 s\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print('Initialized')\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "average_loss = 0\n",
    "step = 0\n",
    "eval_interval = 1000\n",
    "start_time = time.time()\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        num_inputs = len(train_data)\n",
    "        order = np.arange(num_inputs)\n",
    "        np.random.shuffle(order)\n",
    "        for j in range(0, num_inputs, batch_size):\n",
    "            step += 1\n",
    "            batch_index = order[j: j + batch_size]\n",
    "            batch_inputs = train_data[batch_index]\n",
    "            batch_labels = train_labels[batch_index]\n",
    "            feed_dict = {inputs_: batch_inputs, \\\n",
    "                         labels_: batch_labels}\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([train_step, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "            # if step == 100:\n",
    "            #     break\n",
    "\n",
    "            if step % eval_interval == 0:\n",
    "                average_loss /= eval_interval\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at epoch ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print('\\ntime: {:.2f} s'.format(end_time - start_time))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/instance-context-model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_path = './models/'\n",
    "model_name = 'instance-context-model'\n",
    "saver.save(session, os.path.join(model_base_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
